# -*- coding: utf-8 -*-
"""ADM_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Br2p8Q7tUSvO6ODgxLXuXK9wjKjb4iXI
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset
df = pd.read_csv("/content/17_student_performance_estimation.csv")

# Display basic info
print("\n=== Dataset Information ===")
print(df.info())

print("\n=== First 5 Rows ===")
print(df.head())

# Data Preprocessing
print("\n=== Data Preprocessing ===")
# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Create pass/fail status (assuming final_grade is numeric)
df['pass_status'] = np.where(df['final_grade'] >= 60, 1, 0)  # 1=Pass, 0=Fail

# Encode all categorical variables
le = LabelEncoder()
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Identify numeric columns for correlation
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Prepare features and target
X = df.drop(['student_id', 'final_grade', 'pass_status'], axis=1)
y = df['pass_status']

# Exploratory Data Analysis (EDA)
print("\n=== Exploratory Data Analysis ===")

# Plot distribution of final grades
plt.figure(figsize=(10, 6))
sns.histplot(df['final_grade'], kde=True, bins=20)
plt.title("Distribution of Final Grades")
plt.xlabel("Final Grade")
plt.ylabel("Count")
plt.show()

# Plot pass/fail distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='pass_status', data=df)
plt.title("Pass/Fail Distribution")
plt.xticks([0, 1], ['Fail', 'Pass'])
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df[numeric_cols].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Feature Correlation Matrix")
plt.show()

# Impact of school support on pass rate
plt.figure(figsize=(8, 5))
sns.barplot(x='school_support', y='pass_status', data=df)
plt.title("Pass Rate by School Support")
plt.ylabel("Pass Rate")
plt.show()

# Model Building
print("\n=== Model Building ===")

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Initialize and train Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("\n=== Model Evaluation ===")
print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.0f}%")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Feature Importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print("\nFeature Importance:")
print(feature_importance)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis')
plt.title("Feature Importance")
plt.show()

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Fail', 'Predicted Pass'],
            yticklabels=['Actual Fail', 'Actual Pass'])
plt.title("Confusion Matrix")
plt.show()